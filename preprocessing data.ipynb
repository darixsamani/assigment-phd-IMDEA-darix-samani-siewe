{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132b0d85",
   "metadata": {},
   "source": [
    "### Cleaning  Data \n",
    "\n",
    "The propulse of this notebook is to preprocessiong and cleanig our data\n",
    "\n",
    "### Description of data\n",
    "\n",
    "    Data we used conatins many information but for this assigment we just use a few of data. our dataset containsthe following field:\n",
    "    \n",
    " - Square id: identification string of a given square of Milan/Trentino GRID; \n",
    " - Time Interval: start interval time expressed in milliseconds. The end interval time can be obtained by\n",
    "adding 600,000 milliseconds (10 min) to this value;\n",
    " - Country code: the phone country code of the nation\n",
    " - SMS-in activity: activity proportional to the amount of received SMSs inside a given Square id and\n",
    "during a given Time interval. The SMSs are sent from the nation identified by the Country code;  \n",
    " - SMS-out activity: activity proportional to the amount of sent SMSs inside a given Square id during a\n",
    "given Time interval. The SMSs are received in the nation identified by the Country code; \n",
    " - Call-in activity: activity proportional to the amount of received calls inside the Square id during a given\n",
    "Time interval. The calls are issued from the nation identified by the Country code;\n",
    " - Call-out activity: activity proportional to the amount of issued calls inside a given Square id during a\n",
    "given Time interval. The calls are received in the nation identified by the Country code;\n",
    " - Internet traffic activity: number of CDRs generated inside a given Square id during a given Time\n",
    "interval. The Internet traffic is initiated from the nation identified by the Country code;\n",
    "\n",
    " \n",
    " For our study we are just interecting in field like : __Time_interval__, __square id__ and __internet Traffic activity__. the main propulse of this notebook is to preprocessing our dataset and handling the missging value> note that for the internet traffic activity missing value means that there are no activity during this time. That why for the missing value we will just replace by 0\n",
    " \n",
    " ### Requirement for this notebook\n",
    " \n",
    "    The fact that our dataset is very big the fats procesing we will use dask whcih are able to handle the task using parallleling processing. the folling packaage are requires to executes this notebooks:\n",
    "   - dask\n",
    "   - pandas\n",
    "   - numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd6b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuce/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/linuce/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Bottleneck unit testing available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuce/.local/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## import requirements librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7a2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "## load dataset\n",
    "## in this section we just load our dataset into to dataframe pandas\n",
    "\n",
    "## list of clumns of our dataset\n",
    "column_names = [\n",
    "    \"square_id\", \"start_time_ms\", \"country_code\", \n",
    "    \"sms_in_activity\", \"sms_out_activity\", \n",
    "    \"call_in_activity\", \"call_out_activity\", \n",
    "    \"internet_traffic_activity\"\n",
    "]\n",
    "\n",
    "def read_data_set(path: str, selected_columns: List[str]):\n",
    "    \"\"\"\n",
    "    the propuse for this function is to read our data set given path of dataset\n",
    "    input:\n",
    "        path; folder of our dataset\n",
    "        colnames; the list for columsn names of our dataset\n",
    "\n",
    "    oupput: the dataframe of our dataset:\n",
    "    \"\"\"\n",
    "\n",
    "    df = dd.read_csv(path, sep=\"\\t\", header=None, names = [\n",
    "    \"square_id\", \"start_time_ms\", \"country_code\", \n",
    "    \"sms_in_activity\", \"sms_out_activity\", \n",
    "    \"call_in_activity\", \"call_out_activity\", \n",
    "    \"internet_traffic_activity\"\n",
    "])\n",
    "    df = df.compute()\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df[selected_columns]\n",
    "\n",
    "def aggregate_data(df):\n",
    "    \"\"\"\n",
    "    Aggregates data by square_id and time_interval, sums the values, and resets the index.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to be aggregated.\n",
    "    \n",
    "    Returns:\n",
    "    An aggregated DataFrame with time_interval converted to datetime and set as index.\n",
    "    \"\"\"\n",
    "    df = df.groupby(['square_id', 'start_time_ms']).sum().reset_index()\n",
    "    df['start_time'] = pd.to_datetime(df['start_time_ms'], unit=\"ms\")\n",
    "    df.set_index('start_time', inplace=True)\n",
    "    return df\n",
    "\n",
    "def resample_frequency(df, frequency = \"h\"):\n",
    "    \"\"\"\n",
    "    The propulse of this section is to resample our dataset by frequencies and summing internet traffic in this\n",
    "    section\n",
    "    \n",
    "    input:\n",
    "        - df; dataframe for resample\n",
    "        - frequency: the frequency for resample default value H(Hourly)\n",
    "    \"\"\"\n",
    "     \n",
    "    # Group by 'group_id', resample by hour ('H'), and sum the ''\n",
    "    result = df.groupby('square_id').resample('h')['internet_traffic_activity'].sum().reset_index()\n",
    "    return result\n",
    "\n",
    "def preprocessing_data_set(file_path, selected_columns, destination_path):\n",
    "    \n",
    "    df = read_data_set(file_path, selected_columns)\n",
    "    df = aggregate_data(df)\n",
    "    df = resample_frequency(df)\n",
    "    df.to_csv(destination_path, index=False)\n",
    "    return None\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ddde7",
   "metadata": {},
   "source": [
    "## Execution of our proposed solution for cleaning data\n",
    "\n",
    "The propulse of this section is to execute all the procedure we are defined avove in order to preproceesing our data and clean our data. This task take us many taime because our coomputer is not perform well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3a98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = [\"square_id\", \"start_time_ms\",\"internet_traffic_activity\"] ## defined the selcted columns\n",
    "\n",
    "preprocessing_data_set(\"./dataset/*.txt\", selected_columns, \"./data_set_final.csv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008fa89",
   "metadata": {},
   "source": [
    "## Conclusion of this section\n",
    "\n",
    "At the end of this part of our solution we have see that for cleaning data and precessiing our data it very important to use appropriate tools to do that due the massive volume of dataset. We imploy a technique of parallelisme using dash to afficient load our data. But we recommand to use tools like pypark to imporve the performace and time execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
